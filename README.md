# Machine Translation Using Transformer From Scratch


This project implements a Transformer architecture for machine translation, specifically translating Portuguese to English, using TensorFlow.

---

## 🧠 Overview

The Transformer model revolutionizes Natural Language Processing (NLP) by capturing long-range dependencies efficiently and processing sequences in parallel. This repository provides a complete implementation, from model components to training and evaluation.

---

## 🔑 Key Components

- **Positional Encoding**: Enables the model to capture token positions in sequences.
- **Multi-Head Attention**: Allows the model to focus on different parts of the input simultaneously.
- **Encoder-Decoder Layers**: Facilitates the translation process by encoding inputs and decoding outputs.
- **Scaled Dot-Product Attention**: Computes attention weights to prioritize relevant parts of the sequence.

---

## 📚 Dataset

- Utilizes the TED Talks Portuguese-to-English dataset.

---

## 🚀 Training and Optimization

- Custom learning rate schedules  
- Masking techniques for improved training stability

---

## 📈 Results

- Achieved accurate translation with clear attention visualizations highlighting the model's decision-making.

---

## 🛠 Installation

Clone this repository and install dependencies:

```bash
git clone [repository_url]
cd Translation-Transformers
pip install -r requirements.txt
```

---

## ▶️ Usage

Run the provided notebooks and scripts for training and inference:

```bash
git clone [repository_url]
cd Translation-Transformers
pip install -r requirements.txt
```

---

## 📄 License

Distributed under the MIT License. See `LICENSE` for more information.

---

## 🤝 Contributions

Contributions and suggestions are welcome! Please open an issue or submit a pull request.
