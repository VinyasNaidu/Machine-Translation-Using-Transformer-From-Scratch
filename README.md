# Machine Translation Using Transformer From Scratch


This project implements a Transformer architecture for machine translation, specifically translating Portuguese to English, using TensorFlow.

---

## ğŸ§  Overview

The Transformer model revolutionizes Natural Language Processing (NLP) by capturing long-range dependencies efficiently and processing sequences in parallel. This repository provides a complete implementation, from model components to training and evaluation.

---

## ğŸ”‘ Key Components

- **Positional Encoding**: Enables the model to capture token positions in sequences.
- **Multi-Head Attention**: Allows the model to focus on different parts of the input simultaneously.
- **Encoder-Decoder Layers**: Facilitates the translation process by encoding inputs and decoding outputs.
- **Scaled Dot-Product Attention**: Computes attention weights to prioritize relevant parts of the sequence.

---

## ğŸ“š Dataset

- Utilizes the TED Talks Portuguese-to-English dataset.

---

## ğŸš€ Training and Optimization

- Custom learning rate schedules  
- Masking techniques for improved training stability

---

## ğŸ“ˆ Results

- Achieved accurate translation with clear attention visualizations highlighting the model's decision-making.

---

## ğŸ›  Installation

Clone this repository and install dependencies:

```bash
git clone [repository_url]
cd Translation-Transformers
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

Run the provided notebooks and scripts for training and inference:

```bash
git clone [repository_url]
cd Translation-Transformers
pip install -r requirements.txt
```

---

## ğŸ“„ License

Distributed under the MIT License. See `LICENSE` for more information.

---

## ğŸ¤ Contributions

Contributions and suggestions are welcome! Please open an issue or submit a pull request.
